import os
import re
import cv2
import numpy as np
import pandas as pd
from dataclasses import dataclass
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import torchvision
from torchvision.models import resnet50


# =========================
# Config
# =========================
@dataclass
class CFG:
    csv_path: str = "/path/to/your_labels.csv"
    img_root: str = "/path/to/kadid10k"
    img_col: str = "dist_rgb"   # CSV里失真图列名

    # patch + MIL
    crop_size: int = 384
    K: int = 4

    # CPU training
    device: str = "cpu"  # 你现在用CPU；如果后面有GPU改成 "cuda"
    batch_size: int = 2
    accum_steps: int = 8          # 等效 batch = batch_size * accum_steps
    num_workers: int = 0          # CPU+OpenCV 通常 0 更稳；你也可以试 2/4
    epochs: int = 60              # 20 通常不够，建议 60 起步

    # phase
    freeze_epochs: int = 4
    lr_adapter: float = 1e-4
    lr_backbone: float = 1e-5
    weight_decay: float = 1e-4

    # BT.601 range
    yuv_range: str = "limited"    # 你确认是 601；range 不确定就先 limited

    # labels
    label_cols = [
        "halo_y_norm",
        "noise_y_inc",
        "noise_uv_inc",
        "smooth_y_inc",
        "smooth_uv_inc",
        "sharp_gain",
        "pseudo_texture",
        "mos",
    ]
    # 标准化后 weights 更像业务优先级
    label_weights = {
        "halo_y_norm": 1.0,
        "noise_y_inc": 1.0,
        "noise_uv_inc": 1.0,
        "smooth_y_inc": 1.0,
        "smooth_uv_inc": 1.0,
        "sharp_gain": 0.7,
        "pseudo_texture": 1.0,
        "mos": 0.08,
    }

    # multi-task weights
    w_type: float = 0.2   # 25类失真
    w_level: float = 0.1  # 5级强度

    seed: int = 42
    save_path: str = "iqa_kadid_mil_norm_multitask_best.pth"


# =========================
# Utils
# =========================
def set_seed(seed: int):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

def parse_kadid_dist_name(path_or_name: str):
    """
    Example: I01_17_03.png
    returns:
      ref_id (1..81),
      dist_type_idx (0..24),
      level_idx (0..4)
    """
    stem = os.path.splitext(os.path.basename(path_or_name))[0]
    m = re.match(r"^I(\d+)_([0-9]+)_([0-9]+)$", stem)
    if not m:
        raise ValueError(f"Not a KADID distorted name: {stem}")
    ref_id = int(m.group(1))
    dist_type = int(m.group(2))
    level = int(m.group(3))
    return ref_id, dist_type - 1, level - 1


def group_split_by_refid(df: pd.DataFrame, img_col: str, seed: int, train_ratio: float = 0.9):
    ref_ids = []
    for p in df[img_col].astype(str).tolist():
        rid, _, _ = parse_kadid_dist_name(p)
        ref_ids.append(rid)
    df = df.copy()
    df["_ref_id"] = ref_ids

    uniq = df["_ref_id"].unique()
    rng = np.random.RandomState(seed)
    rng.shuffle(uniq)
    n_tr = int(len(uniq) * train_ratio)
    tr_set = set(uniq[:n_tr])

    df_tr = df[df["_ref_id"].isin(tr_set)].copy()
    df_va = df[~df["_ref_id"].isin(tr_set)].copy()
    return df_tr.drop(columns=["_ref_id"]), df_va.drop(columns=["_ref_id"])


# =========================
# BT.601 BGR -> full-res Y,U,V  (uint8, HxW)
# (然后在 patch 上模拟 4:2:0：U/V 先降采样再上采样)
# =========================
def bgr_to_yuv_bt601_fullres(bgr: np.ndarray, range_mode: str = "limited"):
    rgb = bgr[..., ::-1].astype(np.float32)
    R = rgb[..., 0]
    G = rgb[..., 1]
    B = rgb[..., 2]

    if range_mode == "full":
        Y = 0.299 * R + 0.587 * G + 0.114 * B
        U = -0.168736 * R - 0.331264 * G + 0.5 * B + 128.0
        V = 0.5 * R - 0.418688 * G - 0.081312 * B + 128.0
    elif range_mode == "limited":
        Y = 16.0 + (65.481 * R + 128.553 * G + 24.966 * B) / 255.0
        U = 128.0 + (-37.797 * R - 74.203 * G + 112.0 * B) / 255.0
        V = 128.0 + (112.0 * R - 93.786 * G - 18.214 * B) / 255.0
    else:
        raise ValueError(f"Unknown range_mode: {range_mode}")

    Y = np.clip(Y, 0, 255).astype(np.uint8)
    U = np.clip(U, 0, 255).astype(np.uint8)
    V = np.clip(V, 0, 255).astype(np.uint8)
    return Y, U, V


def rand_crop_coords(h: int, w: int, crop: int):
    if h < crop or w < crop:
        raise ValueError(f"Image too small for crop={crop}: got {w}x{h}")
    y0 = 0 if h == crop else np.random.randint(0, h - crop + 1)
    x0 = 0 if w == crop else np.random.randint(0, w - crop + 1)
    return y0, x0


def yuv_patch_to_tensor(Y: np.ndarray, U: np.ndarray, V: np.ndarray, y0: int, x0: int, crop: int):
    """
    从 full-res Y/U/V 裁 patch，然后在 patch 内模拟 4:2:0：
      U/V: crop -> downsample to (crop/2) -> upsample back to crop
    输出: (3,crop,crop) float32 in [0,1] channels=[Y,U,V]
    """
    y_patch = Y[y0:y0+crop, x0:x0+crop]
    u_patch = U[y0:y0+crop, x0:x0+crop]
    v_patch = V[y0:y0+crop, x0:x0+crop]

    # simulate 4:2:0 (NV12 like): down then up
    u_half = cv2.resize(u_patch, (crop // 2, crop // 2), interpolation=cv2.INTER_AREA)
    v_half = cv2.resize(v_patch, (crop // 2, crop // 2), interpolation=cv2.INTER_AREA)
    u_full = cv2.resize(u_half, (crop, crop), interpolation=cv2.INTER_LINEAR)
    v_full = cv2.resize(v_half, (crop, crop), interpolation=cv2.INTER_LINEAR)

    x = np.stack([y_patch, u_full, v_full], axis=0).astype(np.float32) / 255.0
    return torch.from_numpy(x)


# =========================
# Dataset (MIL + label norm + parse type/level)
# =========================
class IQADatasetKADIDMIL(Dataset):
    def __init__(self, df: pd.DataFrame, cfg: CFG, train: bool, y_mean: np.ndarray, y_std: np.ndarray):
        self.df = df.reset_index(drop=True)
        self.cfg = cfg
        self.train = train

        self.paths = self.df[cfg.img_col].astype(str).tolist()

        self.y_raw = self.df[cfg.label_cols].astype(np.float32).values
        self.y_mean = y_mean.astype(np.float32)
        self.y_std = y_std.astype(np.float32)

        self.y_norm = (self.y_raw - self.y_mean[None, :]) / (self.y_std[None, :] + 1e-6)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        rel_path = self.paths[idx]
        img_path = os.path.join(self.cfg.img_root, rel_path)

        bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)
        if bgr is None:
            raise ValueError(f"Failed to read image: {img_path}")

        if self.train and np.random.rand() < 0.5:
            bgr = cv2.flip(bgr, 1)

        # parse type/level from name: Ixx_yy_zz
        ref_id, type_idx, level_idx = parse_kadid_dist_name(rel_path)

        # one-time convert full-res YUV
        Y, U, V = bgr_to_yuv_bt601_fullres(bgr, range_mode=self.cfg.yuv_range)

        h, w = Y.shape
        crop = self.cfg.crop_size

        # sample K patches
        xs = []
        for _ in range(self.cfg.K):
            y0, x0 = rand_crop_coords(h, w, crop)
            xs.append(yuv_patch_to_tensor(Y, U, V, y0, x0, crop))

        xk = torch.stack(xs, dim=0)  # (K,3,crop,crop)

        y_norm = torch.from_numpy(self.y_norm[idx])  # (heads,)
        y_raw = torch.from_numpy(self.y_raw[idx])    # (heads,)

        type_idx = torch.tensor(type_idx, dtype=torch.long)
        level_idx = torch.tensor(level_idx, dtype=torch.long)
        return xk, y_norm, y_raw, type_idx, level_idx


# =========================
# Model: Adapter + ResNet50 + heads (reg + type + level)
# MIL mean pooling on feature or logits
# =========================
IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)

class YUVtoRGBAdapter(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 3, kernel_size=1, bias=True)
        self.bn = nn.BatchNorm2d(3)
        with torch.no_grad():
            self.conv.weight.zero_()
            self.conv.bias.zero_()
            for c in range(3):
                self.conv.weight[c, c, 0, 0] = 1.0

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return torch.sigmoid(x)  # [0,1]

class MultiTaskIQAResNetMIL(nn.Module):
    def __init__(self, num_reg_heads: int, num_types: int = 25, num_levels: int = 5):
        super().__init__()
        self.adapter = YUVtoRGBAdapter()

        backbone = resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)
        self.backbone = backbone
        self.backbone.fc = nn.Identity()  # -> 2048

        self.reg_head = nn.Sequential(
            nn.Linear(2048, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(512, num_reg_heads),
        )
        self.type_head = nn.Linear(2048, num_types)
        self.level_head = nn.Linear(2048, num_levels)

    def forward(self, xk):
        """
        xk: (B,K,3,H,W) or (B,3,H,W)
        returns:
          reg_out: (B,heads)
          type_logits: (B,25)
          level_logits:(B,5)
        """
        if xk.dim() == 4:
            x = xk
            B = x.size(0)
            K = 1
        else:
            B, K, C, H, W = xk.shape
            x = xk.view(B * K, C, H, W)

        x = self.adapter(x)
        mean = IMAGENET_MEAN.to(x.device)
        std = IMAGENET_STD.to(x.device)
        x = (x - mean) / std

        feat = self.backbone(x)  # (B*K,2048)

        reg = self.reg_head(feat)       # (B*K,heads)
        tlog = self.type_head(feat)     # (B*K,25)
        llog = self.level_head(feat)    # (B*K,5)

        if K > 1:
            reg = reg.view(B, K, -1).mean(dim=1)
            tlog = tlog.view(B, K, -1).mean(dim=1)
            llog = llog.view(B, K, -1).mean(dim=1)

        return reg, tlog, llog


# =========================
# Freeze / Unfreeze
# =========================
def set_backbone_trainable(model: MultiTaskIQAResNetMIL, mode: str):
    for p in model.backbone.parameters():
        p.requires_grad = False

    if mode == "freeze_all":
        return
    if mode == "unfreeze_top":
        for p in model.backbone.layer3.parameters():
            p.requires_grad = True
        for p in model.backbone.layer4.parameters():
            p.requires_grad = True
        return
    if mode == "unfreeze_all":
        for p in model.backbone.parameters():
            p.requires_grad = True
        return
    raise ValueError(f"Unknown mode: {mode}")

def build_optimizer(model: MultiTaskIQAResNetMIL, cfg: CFG):
    pg_adapter = list(model.adapter.parameters()) \
                 + list(model.reg_head.parameters()) \
                 + list(model.type_head.parameters()) \
                 + list(model.level_head.parameters())

    pg_backbone = [p for p in model.backbone.parameters() if p.requires_grad]

    params = [{"params": pg_adapter, "lr": cfg.lr_adapter}]
    if len(pg_backbone) > 0:
        params.append({"params": pg_backbone, "lr": cfg.lr_backbone})
    return torch.optim.AdamW(params, weight_decay=cfg.weight_decay)


# =========================
# Loss / Train / Valid
# =========================
def weighted_smooth_l1(pred, target, w):
    loss = nn.functional.smooth_l1_loss(pred, target, reduction="none")
    return (loss * w).mean()

def accuracy(logits, target):
    pred = torch.argmax(logits, dim=1)
    return (pred == target).float().mean().item()

def train_one_epoch(model, loader, optimizer, w_reg, cfg: CFG, ce):
    model.train()
    total_loss = 0.0
    n = 0

    optimizer.zero_grad(set_to_none=True)

    for step, (xk, y_norm, _y_raw, type_idx, level_idx) in enumerate(tqdm(loader, desc="train", leave=False)):
        xk = xk.to(cfg.device)
        y_norm = y_norm.to(cfg.device)
        type_idx = type_idx.to(cfg.device)
        level_idx = level_idx.to(cfg.device)

        reg_pred, tlog, llog = model(xk)

        loss_reg = weighted_smooth_l1(reg_pred, y_norm, w_reg)
        loss_type = ce(tlog, type_idx)
        loss_level = ce(llog, level_idx)

        loss = loss_reg + cfg.w_type * loss_type + cfg.w_level * loss_level
        loss = loss / cfg.accum_steps
        loss.backward()

        if (step + 1) % cfg.accum_steps == 0:
            optimizer.step()
            optimizer.zero_grad(set_to_none=True)

        total_loss += float(loss.item()) * xk.size(0) * cfg.accum_steps
        n += xk.size(0)

    # 如果最后没整除，补一步
    if (step + 1) % cfg.accum_steps != 0:
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)

    return total_loss / max(n, 1)

@torch.no_grad()
def valid_one_epoch(model, loader, w_reg, cfg: CFG, y_mean: np.ndarray, y_std: np.ndarray, label_cols, ce):
    model.eval()
    total_loss = 0.0
    n = 0

    mae_sum = np.zeros(len(label_cols), dtype=np.float64)
    type_acc_sum = 0.0
    level_acc_sum = 0.0

    y_mean_t = torch.from_numpy(y_mean).to(cfg.device).view(1, -1)
    y_std_t = torch.from_numpy(y_std).to(cfg.device).view(1, -1)

    for xk, y_norm, y_raw, type_idx, level_idx in tqdm(loader, desc="valid", leave=False):
        xk = xk.to(cfg.device)
        y_norm = y_norm.to(cfg.device)
        y_raw = y_raw.to(cfg.device)
        type_idx = type_idx.to(cfg.device)
        level_idx = level_idx.to(cfg.device)

        reg_pred, tlog, llog = model(xk)

        loss_reg = weighted_smooth_l1(reg_pred, y_norm, w_reg)
        loss_type = ce(tlog, type_idx)
        loss_level = ce(llog, level_idx)
        loss = loss_reg + cfg.w_type * loss_type + cfg.w_level * loss_level

        # MAE in original units
        pred_raw = reg_pred * (y_std_t + 1e-6) + y_mean_t
        mae = torch.mean(torch.abs(pred_raw - y_raw), dim=0)  # (heads,)
        mae_sum += mae.cpu().numpy() * xk.size(0)

        type_acc_sum += accuracy(tlog, type_idx) * xk.size(0)
        level_acc_sum += accuracy(llog, level_idx) * xk.size(0)

        total_loss += float(loss.item()) * xk.size(0)
        n += xk.size(0)

    loss_avg = total_loss / max(n, 1)
    mae_avg = mae_sum / max(n, 1)
    type_acc = type_acc_sum / max(n, 1)
    level_acc = level_acc_sum / max(n, 1)
    return loss_avg, mae_avg, type_acc, level_acc


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    df = pd.read_csv(cfg.csv_path)

    need_cols = [cfg.img_col] + cfg.label_cols
    miss = [c for c in need_cols if c not in df.columns]
    if miss:
        raise ValueError(f"CSV missing columns: {miss}")

    # split by ref_id parsed from filename (no leakage)
    df_tr, df_va = group_split_by_refid(df, cfg.img_col, cfg.seed, train_ratio=0.9)
    print(f"[split] by ref_id(parsed): train={len(df_tr)} valid={len(df_va)}")

    # label normalization stats from train only
    y_tr = df_tr[cfg.label_cols].astype(np.float32).values
    y_mean = y_tr.mean(axis=0)
    y_std = y_tr.std(axis=0)
    y_std = np.maximum(y_std, 1e-3).astype(np.float32)

    ds_tr = IQADatasetKADIDMIL(df_tr, cfg, train=True,  y_mean=y_mean, y_std=y_std)
    ds_va = IQADatasetKADIDMIL(df_va, cfg, train=False, y_mean=y_mean, y_std=y_std)

    dl_tr = DataLoader(
        ds_tr,
        batch_size=cfg.batch_size,
        shuffle=True,
        num_workers=cfg.num_workers,
        pin_memory=False,   # CPU
        drop_last=True
    )
    dl_va = DataLoader(
        ds_va,
        batch_size=cfg.batch_size,
        shuffle=False,
        num_workers=cfg.num_workers,
        pin_memory=False
    )

    model = MultiTaskIQAResNetMIL(num_reg_heads=len(cfg.label_cols)).to(cfg.device)

    # regression weights tensor
    w_reg = torch.tensor([cfg.label_weights[c] for c in cfg.label_cols],
                         device=cfg.device).view(1, -1)

    ce = nn.CrossEntropyLoss()

    best = 1e9

    # Phase1: freeze backbone
    set_backbone_trainable(model, "freeze_all")
    optimizer = build_optimizer(model, cfg)

    # scheduler (optional but helpful)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode="min", factor=0.5, patience=3, verbose=True
    )

    for ep in range(1, cfg.epochs + 1):
        # Phase2: switch ONCE
        if ep == cfg.freeze_epochs + 1:
            set_backbone_trainable(model, "unfreeze_top")
            optimizer = build_optimizer(model, cfg)
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode="min", factor=0.5, patience=3, verbose=True
            )
            print("[phase] switched to unfreeze_top + rebuilt optimizer/scheduler")

        tr_loss = train_one_epoch(model, dl_tr, optimizer, w_reg, cfg, ce)
        va_loss, va_mae, type_acc, level_acc = valid_one_epoch(
            model, dl_va, w_reg, cfg, y_mean, y_std, cfg.label_cols, ce
        )

        print(f"Epoch {ep:03d} | train {tr_loss:.6f} | valid {va_loss:.6f} | type_acc {type_acc:.3f} | level_acc {level_acc:.3f}")
        mae_str = " | ".join([f"{name}:{va_mae[i]:.4g}" for i, name in enumerate(cfg.label_cols)])
        print("  MAE:", mae_str)

        scheduler.step(va_loss)

        if va_loss < best:
            best = va_loss
            torch.save(
                {
                    "model": model.state_dict(),
                    "cfg": cfg.__dict__,
                    "label_cols": cfg.label_cols,
                    "y_mean": y_mean,
                    "y_std": y_std,
                },
                cfg.save_path
            )
            print(f"  -> saved {cfg.save_path}")

    print("Done. Best valid:", best)


if __name__ == "__main__":
    main()
