import os
import cv2
import numpy as np
import pandas as pd
from dataclasses import dataclass
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import torchvision
from torchvision.models import resnet50


# =========================
# Config
# =========================
@dataclass
class CFG:
    csv_path: str = "/path/to/kadid_labels_rgb_only_nv12_v2.csv"
    img_root: str = "/path/to/kadid10k"
    img_col: str = "dist_rgb"

    crop_size: int = 384
    batch_size: int = 16
    num_workers: int = 8
    epochs: int = 20

    freeze_epochs: int = 4
    lr_adapter: float = 2e-4
    lr_backbone: float = 2e-5
    weight_decay: float = 1e-4

    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42

    # BT.601 range mode: "limited" or "full"
    # 你只说了 601，我默认用 limited（视频链路常见）；如果你线上是 full range，把它改成 "full"
    yuv_range: str = "limited"

    label_cols = [
        "halo_y_norm",
        "noise_y_inc",
        "noise_uv_inc",
        "smooth_y_inc",
        "smooth_uv_inc",
        "sharp_gain",
        "pseudo_texture",
        "mos",
    ]
    label_weights = {
        "halo_y_norm": 1.0,
        "noise_y_inc": 1.0,
        "noise_uv_inc": 1.0,
        "smooth_y_inc": 0.8,
        "smooth_uv_inc": 0.8,
        "sharp_gain": 0.6,
        "pseudo_texture": 0.8,
        "mos": 0.1,
    }


def set_seed(seed: int):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


# =========================
# Crop (no resize): random 384x384
# =========================
def random_crop_bgr(bgr: np.ndarray, crop: int):
    h, w = bgr.shape[:2]
    if h < crop or w < crop:
        raise ValueError(f"Image too small for crop {crop}: got {w}x{h}")

    y0 = 0 if h == crop else np.random.randint(0, h - crop + 1)
    x0 = 0 if w == crop else np.random.randint(0, w - crop + 1)
    return bgr[y0:y0+crop, x0:x0+crop]


# =========================
# Explicit BT.601 RGB->YUV420(NV12)
# =========================
def bgr_to_nv12_bt601(bgr: np.ndarray, range_mode: str = "limited"):
    """
    Input:
      bgr uint8 (H,W,3)
    Output (NV12 planes):
      y  uint8 (H,W)
      uv uint8 (H/2, W) interleaved U,V
    Notes:
      - This uses explicit BT.601 formulas.
      - range_mode:
          "limited": Y in [16..235], U/V in [16..240] style (video)
          "full":    Y/U/V in [0..255] style
    """
    # BGR -> RGB float
    rgb = bgr[..., ::-1].astype(np.float32)
    R = rgb[..., 0]
    G = rgb[..., 1]
    B = rgb[..., 2]

    if range_mode == "full":
        # BT.601 full range
        Y = 0.299 * R + 0.587 * G + 0.114 * B
        U = -0.168736 * R - 0.331264 * G + 0.5 * B + 128.0
        V = 0.5 * R - 0.418688 * G - 0.081312 * B + 128.0
    elif range_mode == "limited":
        # BT.601 limited range (studio swing)
        # Common digital video coefficients
        Y = 16.0 + (65.481 * R + 128.553 * G + 24.966 * B) / 255.0
        U = 128.0 + (-37.797 * R - 74.203 * G + 112.0 * B) / 255.0
        V = 128.0 + (112.0 * R - 93.786 * G - 18.214 * B) / 255.0
    else:
        raise ValueError(f"Unknown range_mode: {range_mode}")

    Y = np.clip(Y, 0, 255).astype(np.uint8)
    U = np.clip(U, 0, 255).astype(np.uint8)
    V = np.clip(V, 0, 255).astype(np.uint8)

    h, w = Y.shape

    # 4:2:0 subsample U/V to half res using area resize (2x2 average)
    U_half = cv2.resize(U, (w // 2, h // 2), interpolation=cv2.INTER_AREA)
    V_half = cv2.resize(V, (w // 2, h // 2), interpolation=cv2.INTER_AREA)

    uv = np.empty((h // 2, w), dtype=np.uint8)
    uv[:, 0::2] = U_half
    uv[:, 1::2] = V_half
    return Y, uv


def nv12_to_yuv3_tensor_noresize(y_u8: np.ndarray, uv_u8: np.ndarray):
    """
    NV12 -> (Y, U_up, V_up) 3ch tensor in [0,1], shape (3,H,W)
    不做 resize：保持输入 crop 的原尺寸（这里就是 384）
    """
    h, w = y_u8.shape
    u = uv_u8[:, 0::2]
    v = uv_u8[:, 1::2]

    # upsample U/V back to full res
    u_full = cv2.resize(u, (w, h), interpolation=cv2.INTER_LINEAR)
    v_full = cv2.resize(v, (w, h), interpolation=cv2.INTER_LINEAR)

    x = np.stack([y_u8, u_full, v_full], axis=0).astype(np.float32) / 255.0
    return torch.from_numpy(x)


# =========================
# Dataset
# =========================
class IQADatasetNV12Crop384(Dataset):
    def __init__(self, df: pd.DataFrame, cfg: CFG, train: bool):
        self.df = df.reset_index(drop=True)
        self.cfg = cfg
        self.train = train

        self.paths = self.df[cfg.img_col].tolist()
        self.labels = self.df[cfg.label_cols].astype(np.float32).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        rel_path = self.paths[idx]
        img_path = os.path.join(self.cfg.img_root, rel_path)

        bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)
        if bgr is None:
            raise ValueError(f"Failed to read image: {img_path}")

        # mild augmentation
        if self.train and np.random.rand() < 0.5:
            bgr = cv2.flip(bgr, 1)

        # crop 384x384 (no resize)
        bgr = random_crop_bgr(bgr, self.cfg.crop_size)

        # BT.601 -> NV12
        y, uv = bgr_to_nv12_bt601(bgr, range_mode=self.cfg.yuv_range)

        # NV12 -> YUV(3ch) tensor, keep 384x384
        x = nv12_to_yuv3_tensor_noresize(y, uv)  # (3,384,384) in [0,1]

        yv = torch.from_numpy(self.labels[idx])  # (heads,)
        return x, yv


# =========================
# Model: YUV -> Adapter -> ResNet50 -> Multi-head
# =========================
IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)

class YUVtoRGBAdapter(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 3, kernel_size=1, bias=True)
        self.bn = nn.BatchNorm2d(3)

        # init near identity
        with torch.no_grad():
            self.conv.weight.zero_()
            self.conv.bias.zero_()
            for c in range(3):
                self.conv.weight[c, c, 0, 0] = 1.0

    def forward(self, x_yuv):
        x = self.conv(x_yuv)
        x = self.bn(x)
        x = torch.sigmoid(x)  # keep in [0,1]
        return x

class MultiHeadIQAWithAdapter(nn.Module):
    def __init__(self, num_heads: int):
        super().__init__()
        self.adapter = YUVtoRGBAdapter()

        backbone = resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)
        self.backbone = backbone
        self.backbone.fc = nn.Identity()  # (B,2048)

        self.head = nn.Sequential(
            nn.Linear(2048, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(512, num_heads),
        )

    def forward(self, x_yuv):
        x = self.adapter(x_yuv)

        mean = IMAGENET_MEAN.to(x.device)
        std = IMAGENET_STD.to(x.device)
        x = (x - mean) / std

        feat = self.backbone(x)   # (B,2048)
        out = self.head(feat)     # (B,heads)
        return out


# =========================
# Freeze / Unfreeze
# =========================
def set_backbone_trainable(model: MultiHeadIQAWithAdapter, mode: str):
    for p in model.backbone.parameters():
        p.requires_grad = False

    if mode == "freeze_all":
        return
    if mode == "unfreeze_top":
        for p in model.backbone.layer3.parameters():
            p.requires_grad = True
        for p in model.backbone.layer4.parameters():
            p.requires_grad = True
        return
    if mode == "unfreeze_all":
        for p in model.backbone.parameters():
            p.requires_grad = True
        return
    raise ValueError(f"Unknown mode: {mode}")


def build_optimizer(model: MultiHeadIQAWithAdapter, cfg: CFG):
    pg_adapter = list(model.adapter.parameters()) + list(model.head.parameters())
    pg_backbone = [p for p in model.backbone.parameters() if p.requires_grad]

    params = [{"params": pg_adapter, "lr": cfg.lr_adapter}]
    if len(pg_backbone) > 0:
        params.append({"params": pg_backbone, "lr": cfg.lr_backbone})

    return torch.optim.AdamW(params, weight_decay=cfg.weight_decay)


# =========================
# Train / Valid
# =========================
def weighted_smooth_l1(pred, target, w):
    loss = nn.functional.smooth_l1_loss(pred, target, reduction="none")
    return (loss * w).mean()

def train_one_epoch(model, loader, optimizer, w, cfg: CFG):
    model.train()
    total, n = 0.0, 0
    for x, y in tqdm(loader, desc="train", leave=False):
        x = x.to(cfg.device, non_blocking=True)
        y = y.to(cfg.device, non_blocking=True)

        pred = model(x)
        loss = weighted_smooth_l1(pred, y, w)

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        total += float(loss.item()) * x.size(0)
        n += x.size(0)
    return total / max(n, 1)

@torch.no_grad()
def valid_one_epoch(model, loader, w, cfg: CFG):
    model.eval()
    total, n = 0.0, 0
    for x, y in tqdm(loader, desc="valid", leave=False):
        x = x.to(cfg.device, non_blocking=True)
        y = y.to(cfg.device, non_blocking=True)

        pred = model(x)
        loss = weighted_smooth_l1(pred, y, w)

        total += float(loss.item()) * x.size(0)
        n += x.size(0)
    return total / max(n, 1)


def main():
    cfg = CFG()
    set_seed(cfg.seed)

    df = pd.read_csv(cfg.csv_path)
    need_cols = [cfg.img_col] + cfg.label_cols
    missing = [c for c in need_cols if c not in df.columns]
    if missing:
        raise ValueError(f"CSV missing columns: {missing}")

    # shuffle & split
    df = df.sample(frac=1.0, random_state=cfg.seed).reset_index(drop=True)
    split = int(len(df) * 0.9)
    df_tr, df_va = df.iloc[:split], df.iloc[split:]

    ds_tr = IQADatasetNV12Crop384(df_tr, cfg, train=True)
    ds_va = IQADatasetNV12Crop384(df_va, cfg, train=False)

    dl_tr = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=True,
                       num_workers=cfg.num_workers, pin_memory=True, drop_last=True)
    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False,
                       num_workers=cfg.num_workers, pin_memory=True)

    model = MultiHeadIQAWithAdapter(num_heads=len(cfg.label_cols)).to(cfg.device)

    w = torch.tensor([cfg.label_weights[c] for c in cfg.label_cols],
                     device=cfg.device).view(1, -1)

    best = 1e9

    for ep in range(1, cfg.epochs + 1):
        if ep <= cfg.freeze_epochs:
            mode = "freeze_all"
        else:
            mode = "unfreeze_top"  # 更稳：只解冻 layer3/layer4

        set_backbone_trainable(model, mode)
        optimizer = build_optimizer(model, cfg)

        tr_loss = train_one_epoch(model, dl_tr, optimizer, w, cfg)
        va_loss = valid_one_epoch(model, dl_va, w, cfg)

        print(f"Epoch {ep:02d} | mode={mode:11s} | train {tr_loss:.6f} | valid {va_loss:.6f}")

        if va_loss < best:
            best = va_loss
            torch.save(
                {"model": model.state_dict(),
                 "cfg": cfg.__dict__,
                 "label_cols": cfg.label_cols},
                "iqa_nv12_bt601_crop384_best.pth"
            )
            print("  -> saved iqa_nv12_bt601_crop384_best.pth")

    print("Done. Best valid:", best)


if __name__ == "__main__":
    main()
